<?xml version="1.0" encoding="UTF-8"?>
<!-- scan：当此属性设置为true时，配置文件如果发生改变，将会被重新加载，默认值为true。 scanPeriod：设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，
	默认单位是毫秒当scan为true时，此属性生效。默认的时间间隔为1分钟。 debug：当此属性设置为true时，将打印出logback内部日志信息，实时查看logback运行状态。
	默认值为false。 -->
<!-- <configuration scan="false" scanPeriod="60 seconds" debug="false"> -->
<!--※※※※※※※※※※※※※※※※※※※日志收集方式之使用logback结合kafka收集到ELK※※※※※※※※※※※※※※※※※※※-->
<configuration>

    <!--设置上下文名称,用于区分不同应用程序的记录。一旦设置不能修改, 可以通过%contextName来打印日志上下文名称 -->
    <contextName>kafka-log-test</contextName>
    <!-- 定义日志的根目录 -->
    <property name="logDir" value="E:/springcloud-log/kafka-log" />
    <!-- 定义日志文件名称 -->
    <property name="logName" value="kafkaLog"></property>


    <!-- ConsoleAppender 表示控制台输出 -->
    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
        <!-- 日志输出格式： %d表示日期时间， %thread表示线程名， %-5level：级别从左显示5个字符宽度, %logger{50}
            表示logger名字最长50个字符，否则按照句点分割。 %msg：日志消息， %n是换行符 -->
        <encoder>
            <pattern>%d{HH:mm:ss.SSS} %contextName [%thread] %-5level %logger{36} - %msg%n</pattern>
        </encoder>
    </appender>

    <!-- 异常错误日志记录到文件  -->
    <appender name="logfile" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <!-- <Encoding>UTF-8</Encoding> -->
        <File>${logDir}/${logName}.log</File>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <FileNamePattern>${logDir}/history/test_log.%d{yyyy-MM-dd}.rar</FileNamePattern>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>%d{HH:mm:ss.SSS} %contextName [%thread] %-5level %logger{36} - %msg%n</pattern>
        </encoder>
    </appender>


    <appender name="kafkaAppender" class="com.github.danielwegener.logback.kafka.KafkaAppender">
        <encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
            <pattern>%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n</pattern>
        </encoder>
        <topic>testlog</topic>
        <!-- we don't care how the log messages will be partitioned  -->
        <keyingStrategy class="com.github.danielwegener.logback.kafka.keying.NoKeyKeyingStrategy" />

        <!-- use async delivery. the application threads are not blocked by logging -->
        <deliveryStrategy class="com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy" />

        <!-- each <producerConfig> translates to regular kafka-client config (format: key=value) -->
        <!-- producer configs are documented here: https://kafka.apache.org/documentation.html#newproducerconfigs -->
        <!-- bootstrap.servers is the only mandatory producerConfig -->
        <producerConfig>bootstrap.servers=localhost:9092</producerConfig>
        <!-- don't wait for a broker to ack the reception of a batch.  -->
        <producerConfig>acks=0</producerConfig>
        <!-- wait up to 1000ms and collect log messages before sending them as a batch -->
        <producerConfig>linger.ms=1000</producerConfig>
        <!-- even if the producer buffer runs full, do not block the application but start to drop messages -->
        <producerConfig>max.block.ms=0</producerConfig>
        <!-- define a client-id that you use to identify yourself against the kafka broker -->
        <producerConfig>client.id=0</producerConfig>

    </appender>


    <root level="debug">
        <appender-ref ref="CONSOLE" />
        <appender-ref ref="logfile"/>
        <appender-ref ref="kafkaAppender" />
    </root>

</configuration>

<!--※※※※※※※※※※※※※※※※※※※日志收集方式之使用logback结合kafka收集到ELK※※※※※※※※※※※※※※※※※※※-->








<!--※※※※※※※※※※※※※※※※※※※日志收集方式之使用logback结合logstash收集到ELK※※※※※※※※※※※※※※※※※※※-->

<!--<configuration>-->

    <!--<appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">-->
    <!--&lt;!&ndash; 日志输出格式： %d表示日期时间， %thread表示线程名， %-5level：级别从左显示5个字符宽度, %logger{50}-->
    <!--表示logger名字最长50个字符，否则按照句点分割。 %msg：日志消息， %n是换行符 &ndash;&gt;-->
    <!--<encoder>-->
    <!--<pattern>%d{HH:mm:ss.SSS} %contextName [%thread] %-5level %logger{36} - %msg%n</pattern>-->
    <!--</encoder>-->
    <!--</appender>-->

    <!--&lt;!&ndash;<include resource="org/springframework/boot/logging/logback/base.xml"/>&ndash;&gt;-->
    <!--<appender name="stash" class="net.logstash.logback.appender.LogstashTcpSocketAppender">-->
        <!--<destination>127.0.0.1:9100</destination>-->
        <!--<includeCallerData>true</includeCallerData>-->

        <!--<encoder class="net.logstash.logback.encoder.LogstashEncoder">-->
            <!--<includeCallerData>true</includeCallerData>-->
        <!--</encoder>-->
    <!--</appender>-->

    <!--<root level="debug">-->
    <!--<appender-ref ref="CONSOLE" />-->
    <!--<appender-ref ref="stash"/>-->
    <!--</root>-->

<!--</configuration>-->







<!--logstash config配置文件内容-->

        <!--input {
        tcp {
        host => "127.0.0.1"
        port => 9100
        mode => "server"
        tags => ["tags"]
        codec => json_lines
        }
        }
        output {
        stdout { codec => rubydebug }
        #输出到es
        elasticsearch { hosts => "127.0.0.1:9200" }
        }-->


<!--※※※※※※※※※※※※※※※※※※※日志收集方式之使用logback结合logstash收集到ELK※※※※※※※※※※※※※※※※※※※-->